# -*- coding: utf-8 -*-
"""travel assistance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Brwr8O-6BBXbp9ruSnkEelA7e-3XDLl8
"""

!pip install -q langchain langchain-community langchain-experimental requests langchain-google-genai duckduckgo-search

import os
import requests
from langchain.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

os.environ["GOOGLE_API_KEY"] = "AIzaSyC1XiI7BuiR9BBb4ApLogtzh2UjPRJIV60"
WEATHER_API_KEY = "5444aaffa0304d6cb3970349251306"

# Initialize Gemini LLM
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

@tool
def get_weather_forecast(destination: str) -> str:
    """Fetches the current weather forecast for a given destination using WeatherAPI.com."""
    base_url = "http://api.weatherapi.com/v1/current.json"
    params = {"key": WEATHER_API_KEY, "q": destination, "aqi": "no"}
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        data = response.json()
        weather = {
            "location": data["location"]["name"],
            "temp_c": data["current"]["temp_c"],
            "condition": data["current"]["condition"]["text"],
            "humidity": data["current"]["humidity"],
            "wind_kph": data["current"]["wind_kph"]
        }
        return (
            f"Weather in {weather['location']}:\n"
            f"Temperature: {weather['temp_c']}Â°C\n"
            f"Condition: {weather['condition']}\n"
            f"Humidity: {weather['humidity']}%\n"
            f"Wind Speed: {weather['wind_kph']} km/h"
        )
    except requests.RequestException as e:
        return f"Error fetching weather data: {str(e)}"

# DuckDuckGo search tool
search_tool = DuckDuckGoSearchRun()

# Combine tools
tools = [get_weather_forecast, search_tool]

# Chat prompt (must use 'input' and MessagesPlaceholder at the end)
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful travel assistant. Use tools to give weather and attractions."),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")  # Required by LangChain agent
])

# Gemini LLM
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

# Create agent
agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)

# Create executor
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

def query_travel_assistant(city: str) -> dict:
    return agent_executor.invoke({"input": f"Give me the weather and top attractions in {city}."})

# Example Usage
destination = input("Enter your destination (e.g., Tokyo): ")
result = query_travel_assistant(destination)
print("\nTravel Assistant Response:")
print(result["output"])

# Test the agent
result = agent_executor.invoke({"destination": "London"})
print(result)